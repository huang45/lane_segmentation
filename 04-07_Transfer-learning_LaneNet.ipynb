{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path as ops\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import glog as log\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warning for tf 2.0\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Data Set and Weights Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/training_data_bdd100k_highway_lane_t5/'\n",
    "weights_path = 'model/tusimple_lanenet/tusimple_lanenet_vgg_2018-10-19-13-33-56.ckpt-200000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "__C = edict()\n",
    "\n",
    "cfg = __C\n",
    "\n",
    "__C.TRAIN = edict()\n",
    "__C.TRAIN.EPOCHS = 10000\n",
    "__C.TRAIN.NOOFBATCHES = 10\n",
    "__C.TRAIN.MOMENTUM = 0.9\n",
    "__C.TRAIN.LEARNING_RATE = 0.0005\n",
    "__C.TRAIN.GPU_MEMORY_FRACTION = 0.85\n",
    "__C.TRAIN.TF_ALLOW_GROWTH = True\n",
    "__C.TRAIN.BATCH_SIZE = 8\n",
    "__C.TRAIN.IMG_HEIGHT = 256\n",
    "__C.TRAIN.IMG_WIDTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, dataset_info_file):\n",
    "        self._gt_img_list, self._gt_label_binary_list = self._init_dataset(dataset_info_file)\n",
    "        self._random_dataset()\n",
    "        self._next_batch_loop_count = 0\n",
    "\n",
    "    def _init_dataset(self, dataset_info_file):\n",
    "        gt_img_list = []\n",
    "        gt_label_binary_list = []\n",
    "\n",
    "        assert ops.exists(dataset_info_file), '{:s}ã€€does not exist'.format(dataset_info_file)\n",
    "\n",
    "        with open(dataset_info_file, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "\n",
    "                gt_img_list.append(info_tmp[0])\n",
    "                gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        return gt_img_list, gt_label_binary_list\n",
    "\n",
    "    def _random_dataset(self):\n",
    "        assert len(self._gt_img_list) == len(self._gt_label_binary_list)\n",
    "\n",
    "        random_idx = np.random.permutation(len(self._gt_img_list))\n",
    "        new_gt_img_list = []\n",
    "        new_gt_label_binary_list = []\n",
    "\n",
    "        for index in random_idx:\n",
    "            new_gt_img_list.append(self._gt_img_list[index])\n",
    "            new_gt_label_binary_list.append(self._gt_label_binary_list[index])\n",
    "\n",
    "        self._gt_img_list = new_gt_img_list\n",
    "        self._gt_label_binary_list = new_gt_label_binary_list\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        assert len(self._gt_label_binary_list) == len(self._gt_img_list)\n",
    "\n",
    "        idx_start = batch_size * self._next_batch_loop_count\n",
    "        idx_end = batch_size * self._next_batch_loop_count + batch_size\n",
    "\n",
    "        if idx_start == 0 and idx_end > len(self._gt_label_binary_list):\n",
    "            raise ValueError('Batch size cannot be larger than the total number of samples', \n",
    "                             idx_end, len(self._gt_label_binary_list))\n",
    "\n",
    "        if idx_end > len(self._gt_label_binary_list):\n",
    "            self._random_dataset()\n",
    "            self._next_batch_loop_count = 0\n",
    "            return self.next_batch(batch_size)\n",
    "        else:\n",
    "            gt_img_list = self._gt_img_list[idx_start:idx_end]\n",
    "            gt_label_binary_list = self._gt_label_binary_list[idx_start:idx_end]\n",
    "\n",
    "            gt_imgs = []\n",
    "            gt_labels_binary = []\n",
    "\n",
    "            for gt_img_path in gt_img_list:\n",
    "                gt_imgs.append(cv2.imread(gt_img_path, cv2.IMREAD_COLOR))\n",
    "\n",
    "            for gt_label_path in gt_label_binary_list:\n",
    "                label_img = cv2.imread(gt_label_path, cv2.IMREAD_COLOR)\n",
    "                label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "                idx = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "                label_binary[idx] = 1\n",
    "                gt_labels_binary.append(label_binary)\n",
    "\n",
    "            self._next_batch_loop_count += 1\n",
    "\n",
    "        return gt_imgs, gt_labels_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_dataset_file = ops.join(dataset_dir, 'train.txt')\n",
    "assert ops.exists(train_dataset_file)\n",
    "train_dataset = DataSet(train_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Base Function\n",
    "## cnn_basenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseModel(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def conv2d(inputdata, out_channel, kernel_size, padding='SAME',\n",
    "               stride=1, w_init=None, b_init=None,\n",
    "               split=1, use_bias=True, data_format='NHWC', name=None):\n",
    "        with tf.variable_scope(name):\n",
    "            in_shape = inputdata.get_shape().as_list()\n",
    "            channel_axis = 3 if data_format == 'NHWC' else 1\n",
    "            in_channel = in_shape[channel_axis]\n",
    "            assert in_channel is not None, \"[Conv2D] Input cannot have unknown channel!\"\n",
    "            assert in_channel % split == 0\n",
    "            assert out_channel % split == 0\n",
    "\n",
    "            padding = padding.upper()\n",
    "\n",
    "            if isinstance(kernel_size, list):\n",
    "                filter_shape = [kernel_size[0], kernel_size[1]] + [in_channel / split, out_channel]\n",
    "            else:\n",
    "                filter_shape = [kernel_size, kernel_size] + [in_channel / split, out_channel]\n",
    "\n",
    "            if isinstance(stride, list):\n",
    "                strides = [1, stride[0], stride[1], 1] if data_format == 'NHWC' \\\n",
    "                    else [1, 1, stride[0], stride[1]]\n",
    "            else:\n",
    "                strides = [1, stride, stride, 1] if data_format == 'NHWC' \\\n",
    "                    else [1, 1, stride, stride]\n",
    "\n",
    "            if w_init is None:\n",
    "                w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            if b_init is None:\n",
    "                b_init = tf.constant_initializer()\n",
    "\n",
    "            w = tf.get_variable('W', filter_shape, initializer=w_init)\n",
    "            b = None\n",
    "\n",
    "            if use_bias:\n",
    "                b = tf.get_variable('b', [out_channel], initializer=b_init)\n",
    "\n",
    "            if split == 1:\n",
    "                conv = tf.nn.conv2d(inputdata, w, strides, padding, data_format=data_format)\n",
    "            else:\n",
    "                inputs = tf.split(inputdata, split, channel_axis)\n",
    "                kernels = tf.split(w, split, 3)\n",
    "                outputs = [tf.nn.conv2d(i, k, strides, padding, data_format=data_format)\n",
    "                           for i, k in zip(inputs, kernels)]\n",
    "                conv = tf.concat(outputs, channel_axis)\n",
    "\n",
    "            ret = tf.identity(tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv, name=name)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(inputdata, name=None):\n",
    "        return tf.nn.relu(features=inputdata, name=name)\n",
    "\n",
    "    @staticmethod\n",
    "    def maxpooling(inputdata, kernel_size, stride=None, \\\n",
    "                   padding='VALID', data_format='NHWC', name=None):\n",
    "        padding = padding.upper()\n",
    "\n",
    "        if stride is None:\n",
    "            stride = kernel_size\n",
    "\n",
    "        if isinstance(kernel_size, list):\n",
    "            kernel = [1, kernel_size[0], kernel_size[1], 1] if data_format == 'NHWC' \\\n",
    "                else [1, 1, kernel_size[0], kernel_size[1]]\n",
    "        else:\n",
    "            kernel = [1, kernel_size, kernel_size, 1] if data_format == 'NHWC' \\\n",
    "                else [1, 1, kernel_size, kernel_size]\n",
    "\n",
    "        if isinstance(stride, list):\n",
    "            strides = [1, stride[0], stride[1], 1] if data_format == 'NHWC' \\\n",
    "                else [1, 1, stride[0], stride[1]]\n",
    "        else:\n",
    "            strides = [1, stride, stride, 1] if data_format == 'NHWC' \\\n",
    "                else [1, 1, stride, stride]\n",
    "\n",
    "        return tf.nn.max_pool(value=inputdata, ksize=kernel, strides=strides, \\\n",
    "                              padding=padding, data_format=data_format, name=name)\n",
    "\n",
    "    @staticmethod\n",
    "    def layerbn(inputdata, is_training, name):\n",
    "        return tf.layers.batch_normalization(inputs=inputdata, training=is_training, name=name)\n",
    "\n",
    "    @staticmethod\n",
    "    def deconv2d(inputdata, out_channel, kernel_size, padding='SAME',\n",
    "                 stride=1, w_init=None, b_init=None,\n",
    "                 use_bias=True, activation=None, data_format='channels_last',\n",
    "                 trainable=True, name=None):\n",
    "        with tf.variable_scope(name):\n",
    "            in_shape = inputdata.get_shape().as_list()\n",
    "            channel_axis = 3 if data_format == 'channels_last' else 1\n",
    "            in_channel = in_shape[channel_axis]\n",
    "            assert in_channel is not None, \"[Deconv2D] Input cannot have unknown channel!\"\n",
    "\n",
    "            padding = padding.upper()\n",
    "\n",
    "            if w_init is None:\n",
    "                w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            if b_init is None:\n",
    "                b_init = tf.constant_initializer()\n",
    "\n",
    "            ret = tf.layers.conv2d_transpose(inputs=inputdata, filters=out_channel,\n",
    "                                             kernel_size=kernel_size,\n",
    "                                             strides=stride, padding=padding,\n",
    "                                             data_format=data_format,\n",
    "                                             activation=activation, use_bias=use_bias,\n",
    "                                             kernel_initializer=w_init,\n",
    "                                             bias_initializer=b_init, trainable=trainable,\n",
    "                                             name=name)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model using conv2d, Relu and Maxpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fcn_decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNDecoder(CNNBaseModel):\n",
    "    def __init__(self, phase):\n",
    "        super(FCNDecoder, self).__init__()\n",
    "\n",
    "    def decode(self, input_tensor_dict, decode_layer_list, name):\n",
    "        ret = dict()\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # score stage 1\n",
    "            input_tensor = input_tensor_dict[decode_layer_list[0]]['data']\n",
    "\n",
    "            score = self.conv2d(inputdata=input_tensor, out_channel=64,\n",
    "                                kernel_size=1, use_bias=False, name='score_origin')\n",
    "            decode_layer_list = decode_layer_list[1:]\n",
    "            for i in range(len(decode_layer_list)):\n",
    "                deconv = self.deconv2d(inputdata=score, out_channel=64, kernel_size=4,\n",
    "                                       stride=2, use_bias=False, name='deconv_{:d}'.format(i + 1))\n",
    "                input_tensor = input_tensor_dict[decode_layer_list[i]]['data']\n",
    "                score = self.conv2d(inputdata=input_tensor, out_channel=64,\n",
    "                                    kernel_size=1, use_bias=False, name='score_{:d}'.format(i + 1))\n",
    "                fused = tf.add(deconv, score, name='fuse_{:d}'.format(i + 1))\n",
    "                score = fused\n",
    "\n",
    "            deconv_final = self.deconv2d(inputdata=score, out_channel=64, kernel_size=16,\n",
    "                                         stride=8, use_bias=False, name='deconv_final')\n",
    "\n",
    "            score_final = self.conv2d(inputdata=deconv_final, out_channel=2,\n",
    "                                      kernel_size=1, use_bias=False, name='score_final')\n",
    "\n",
    "            ret['logits'] = score_final\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg_encoder.py\n",
    "https://github.com/machrisaa/tensorflow-vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Encoder(CNNBaseModel):\n",
    "    def __init__(self, phase):\n",
    "        super(VGG16Encoder, self).__init__()\n",
    "        self._train_phase = tf.constant('train', dtype=tf.string)\n",
    "        self._phase = phase\n",
    "        self._is_training = self._init_phase()\n",
    "\n",
    "    def _init_phase(self):\n",
    "        return tf.equal(self._phase, self._train_phase)\n",
    "\n",
    "    def _conv_stage(self, input_tensor, k_size, out_dims, name,\n",
    "                    stride=1, pad='SAME'):\n",
    "        with tf.variable_scope(name):\n",
    "            conv = self.conv2d(inputdata=input_tensor, out_channel=out_dims,\n",
    "                               kernel_size=k_size, stride=stride,\n",
    "                               use_bias=False, padding=pad, name='conv')\n",
    "\n",
    "            bn = self.layerbn(inputdata=conv, is_training=self._is_training, name='bn')\n",
    "\n",
    "            relu = self.relu(inputdata=bn, name='relu')\n",
    "\n",
    "            return relu\n",
    "\n",
    "    def encode(self, input_tensor, name):\n",
    "        ret = OrderedDict()\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # conv stage 1\n",
    "            conv_1_1 = self._conv_stage(input_tensor=input_tensor, k_size=3, out_dims=64, name='conv1_1')\n",
    "            conv_1_2 = self._conv_stage(input_tensor=conv_1_1, k_size=3, out_dims=64, name='conv1_2')\n",
    "            pool1 = self.maxpooling(inputdata=conv_1_2, kernel_size=2, stride=2, name='pool1')\n",
    "\n",
    "            # conv stage 2\n",
    "            conv_2_1 = self._conv_stage(input_tensor=pool1, k_size=3, out_dims=128, name='conv2_1')\n",
    "            conv_2_2 = self._conv_stage(input_tensor=conv_2_1, k_size=3, out_dims=128, name='conv2_2')\n",
    "            pool2 = self.maxpooling(inputdata=conv_2_2, kernel_size=2, stride=2, name='pool2')\n",
    "\n",
    "            # conv stage 3\n",
    "            conv_3_1 = self._conv_stage(input_tensor=pool2, k_size=3, out_dims=256, name='conv3_1')\n",
    "            conv_3_2 = self._conv_stage(input_tensor=conv_3_1, k_size=3, out_dims=256, name='conv3_2')\n",
    "            conv_3_3 = self._conv_stage(input_tensor=conv_3_2, k_size=3, out_dims=256, name='conv3_3')\n",
    "            pool3 = self.maxpooling(inputdata=conv_3_3, kernel_size=2, stride=2, name='pool3')\n",
    "            ret['pool3'] = dict()\n",
    "            ret['pool3']['data'] = pool3\n",
    "            ret['pool3']['shape'] = pool3.get_shape().as_list()\n",
    "\n",
    "            # conv stage 4\n",
    "            conv_4_1 = self._conv_stage(input_tensor=pool3, k_size=3, out_dims=512, name='conv4_1')\n",
    "            conv_4_2 = self._conv_stage(input_tensor=conv_4_1, k_size=3, out_dims=512, name='conv4_2')\n",
    "            conv_4_3 = self._conv_stage(input_tensor=conv_4_2, k_size=3, out_dims=512, name='conv4_3')\n",
    "            pool4 = self.maxpooling(inputdata=conv_4_3, kernel_size=2, stride=2, name='pool4')\n",
    "            ret['pool4'] = dict()\n",
    "            ret['pool4']['data'] = pool4\n",
    "            ret['pool4']['shape'] = pool4.get_shape().as_list()\n",
    "\n",
    "            # conv stage 5\n",
    "            conv_5_1 = self._conv_stage(input_tensor=pool4, k_size=3, out_dims=512, name='conv5_1')\n",
    "            conv_5_2 = self._conv_stage(input_tensor=conv_5_1, k_size=3, out_dims=512, name='conv5_2')\n",
    "            conv_5_3 = self._conv_stage(input_tensor=conv_5_2, k_size=3, out_dims=512, name='conv5_3')\n",
    "            pool5 = self.maxpooling(inputdata=conv_5_3, kernel_size=2, stride=2, name='pool5')\n",
    "            ret['pool5'] = dict()\n",
    "            ret['pool5']['data'] = pool5\n",
    "            ret['pool5']['shape'] = pool5.get_shape().as_list()\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lanenet_merge_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneNet():\n",
    "    def __init__(self, phase):\n",
    "        super(LaneNet, self).__init__()\n",
    "        self._encoder = VGG16Encoder(phase=phase)\n",
    "        self._decoder = FCNDecoder(phase=phase)\n",
    "        return\n",
    "\n",
    "    def _build_model(self, input_tensor, name):\n",
    "        with tf.variable_scope(name):\n",
    "            # first encode\n",
    "            encode_ret = self._encoder.encode(input_tensor=input_tensor,\n",
    "                                              name='encode')\n",
    "            # second decode\n",
    "            decode_ret = self._decoder.decode(input_tensor_dict=encode_ret,\n",
    "                                              name='decode',\n",
    "                                              decode_layer_list=['pool5',\n",
    "                                                                 'pool4',\n",
    "                                                                 'pool3'])\n",
    "            return decode_ret\n",
    "\n",
    "    def compute_loss(self, input_tensor, binary_label, name):\n",
    "        with tf.variable_scope(name):\n",
    "            # Forward propagation to get logits\n",
    "            inference_ret = self._build_model(input_tensor=input_tensor, name='inference')\n",
    "\n",
    "            # Calculate the binary partition loss function\n",
    "            decode_logits = inference_ret['logits']\n",
    "            binary_label_plain = tf.reshape(binary_label,\n",
    "                                            shape=[binary_label.get_shape().as_list()[0] *\n",
    "                                                   binary_label.get_shape().as_list()[1] *\n",
    "                                                   binary_label.get_shape().as_list()[2]])\n",
    "            # Add class weights\n",
    "            unique_labels, unique_id, counts = tf.unique_with_counts(binary_label_plain)\n",
    "            counts = tf.cast(counts, tf.float32)\n",
    "            inverse_weights = tf.divide(1.0,\n",
    "                                        tf.log(tf.add(tf.divide(tf.constant(1.0), counts),\n",
    "                                                      tf.constant(1.02))))\n",
    "            inverse_weights = tf.gather(inverse_weights, binary_label)\n",
    "            binary_segmenatation_loss = tf.losses.sparse_softmax_cross_entropy(labels=binary_label, \\\n",
    "                                                                               logits=decode_logits, \\\n",
    "                                                                               weights=inverse_weights)\n",
    "            binary_segmenatation_loss = tf.reduce_mean(binary_segmenatation_loss)\n",
    "\n",
    "            ret = {'loss': binary_segmenatation_loss, 'binary_seg_logits': decode_logits}\n",
    "\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define cost, optimizer and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    input_tensor = tf.placeholder(dtype=tf.float32,\n",
    "                                  shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT,\n",
    "                                         CFG.TRAIN.IMG_WIDTH, 3],\n",
    "                                  name='input_tensor')\n",
    "    binary_label = tf.placeholder(dtype=tf.int64,\n",
    "                                  shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT,\n",
    "                                         CFG.TRAIN.IMG_WIDTH, 1],\n",
    "                                         name='binary_label')\n",
    "\n",
    "    # Create the model\n",
    "    phase = tf.placeholder(dtype=tf.string, shape=None, name='net_phase')\n",
    "    net = LaneNet(phase=phase)\n",
    "\n",
    "    # Define loss\n",
    "    compute_ret = net.compute_loss(input_tensor=input_tensor, binary_label=binary_label, name='lanenet_model')\n",
    "    loss = compute_ret['loss']\n",
    "\n",
    "    # Evaluate model\n",
    "    out_logits = compute_ret['binary_seg_logits']\n",
    "    out_logits = tf.nn.softmax(logits=out_logits)\n",
    "    out_logits_out = tf.argmax(out_logits, axis=-1)\n",
    "    out = tf.argmax(out_logits, axis=-1)\n",
    "    out = tf.expand_dims(out, axis=-1)\n",
    "\n",
    "    idx = tf.where(tf.equal(binary_label, 1))\n",
    "    pix_cls_ret = tf.gather_nd(out, idx)\n",
    "    accuracy = tf.count_nonzero(pix_cls_ret)\n",
    "    accuracy = tf.divide(accuracy, tf.cast(tf.shape(pix_cls_ret)[0], tf.int64))\n",
    "\n",
    "    # Define optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(CFG.TRAIN.LEARNING_RATE, global_step,\n",
    "                                               100000, 0.1, staircase=True)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, \\\n",
    "                                               momentum=CFG.TRAIN.MOMENTUM).minimize(loss=loss,\n",
    "                                                                                     var_list=tf.trainable_variables(),\n",
    "                                                                                     global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sess configuration\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n",
    "sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n",
    "sess_config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "sess = tf.Session(config=sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/tusimple_lanenet/lanenet_model.pb'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set tf saver\n",
    "model_save_dir = 'model/tusimple_lanenet'\n",
    "if not ops.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "train_start_time = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time()))\n",
    "model_name = 'tusimple_lanenet_{:s}.ckpt'.format(str(train_start_time))\n",
    "model_save_path = ops.join(model_save_dir, model_name)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.train.write_graph(graph_or_graph_def=sess.graph, logdir='',\n",
    "                     name='{:s}/lanenet_model.pb'.format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tf summary\n",
    "tboard_save_path = 'tboard/tusimple_lanenet_{:s}'.format(str(train_start_time))\n",
    "if not ops.exists(tboard_save_path):\n",
    "    os.makedirs(tboard_save_path)\n",
    "\n",
    "train_accuracy_scalar = tf.summary.scalar(name='train_accuracy', tensor=accuracy)\n",
    "train_loss_scalar = tf.summary.scalar(name='train_loss', tensor=loss)\n",
    "learning_rate_scalar = tf.summary.scalar(name='learning_rate', tensor=learning_rate)\n",
    "merged_summary_op = tf.summary.merge([train_accuracy_scalar, train_loss_scalar, learning_rate_scalar])\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(tboard_save_path)\n",
    "summary_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output image folder\n",
    "image_save_path = 'data_ret/tusimple_lanenet_{:s}'.format(str(train_start_time))\n",
    "if not ops.exists(image_save_path):\n",
    "    os.makedirs(image_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "history = []\n",
    "train_cost_time_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_training_data(gt_imgs, binary_gt_labels):\n",
    "    gt_imgs = [cv2.resize(tmp,\n",
    "                          dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "                          dst=tmp,\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "               for tmp in gt_imgs]\n",
    "    gt_imgs = [tmp - VGG_MEAN for tmp in gt_imgs]\n",
    "\n",
    "    binary_gt_labels = [cv2.resize(tmp,\n",
    "                                   dsize=(CFG.TRAIN.IMG_WIDTH, CFG.TRAIN.IMG_HEIGHT),\n",
    "                                   dst=tmp,\n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "                        for tmp in binary_gt_labels]\n",
    "    binary_gt_labels = [np.expand_dims(tmp, axis=-1) for tmp in binary_gt_labels]\n",
    "    \n",
    "    return gt_imgs, binary_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0329 17:16:18.892105 25180 <ipython-input-20-944ee62c4c96>:19] Restore model from last model checkpoint model/tusimple_lanenet/tusimple_lanenet_vgg_2018-10-19-13-33-56.ckpt-200000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:      0 loss= 104.460274 acc= 0.184618 cost_time= 0.454014s \n",
      "Epoch:     10 loss= 14.426443 acc= 0.389281 cost_time= 0.315226s \n",
      "Epoch:     20 loss= 13.354845 acc= 0.370618 cost_time= 0.318642s \n",
      "Epoch:     30 loss= 9.660359 acc= 0.397946 cost_time= 0.318196s \n",
      "Epoch:     40 loss= 9.473042 acc= 0.269665 cost_time= 0.310930s \n",
      "Epoch:     50 loss= 9.316252 acc= 0.253500 cost_time= 0.309334s \n",
      "Epoch:     60 loss= 5.654065 acc= 0.431213 cost_time= 0.309868s \n",
      "Epoch:     70 loss= 5.128681 acc= 0.331378 cost_time= 0.307139s \n",
      "Epoch:     80 loss= 7.096038 acc= 0.293718 cost_time= 0.310485s \n",
      "Epoch:     90 loss= 4.783779 acc= 0.347611 cost_time= 0.312565s \n",
      "Epoch:    100 loss= 7.387997 acc= 0.290838 cost_time= 0.306561s \n",
      "Epoch:    110 loss= 5.352378 acc= 0.341002 cost_time= 0.306669s \n",
      "Epoch:    120 loss= 4.945259 acc= 0.290604 cost_time= 0.308293s \n",
      "Epoch:    130 loss= 4.429411 acc= 0.309449 cost_time= 0.305395s \n",
      "Epoch:    140 loss= 6.117048 acc= 0.400909 cost_time= 0.302798s \n",
      "Epoch:    150 loss= 4.861227 acc= 0.220295 cost_time= 0.304262s \n",
      "Epoch:    160 loss= 5.152537 acc= 0.323620 cost_time= 0.305523s \n",
      "Epoch:    170 loss= 4.751011 acc= 0.320564 cost_time= 0.305533s \n",
      "Epoch:    180 loss= 4.990980 acc= 0.329574 cost_time= 0.304000s \n",
      "Epoch:    190 loss= 5.571103 acc= 0.394242 cost_time= 0.305231s \n",
      "Epoch:    200 loss= 4.704943 acc= 0.327691 cost_time= 0.305833s \n",
      "Epoch:    210 loss= 4.682023 acc= 0.345465 cost_time= 0.309274s \n",
      "Epoch:    220 loss= 4.756532 acc= 0.296117 cost_time= 0.318092s \n",
      "Epoch:    230 loss= 5.194523 acc= 0.328596 cost_time= 0.306040s \n",
      "Epoch:    240 loss= 4.739840 acc= 0.226429 cost_time= 0.304351s \n",
      "Epoch:    250 loss= 4.686624 acc= 0.304549 cost_time= 0.306666s \n",
      "Epoch:    260 loss= 4.472705 acc= 0.395652 cost_time= 0.307427s \n",
      "Epoch:    270 loss= 5.092473 acc= 0.346141 cost_time= 0.305477s \n",
      "Epoch:    280 loss= 4.951454 acc= 0.284783 cost_time= 0.301950s \n",
      "Epoch:    290 loss= 3.983181 acc= 0.374263 cost_time= 0.304857s \n",
      "Epoch:    300 loss= 4.640523 acc= 0.337956 cost_time= 0.305089s \n",
      "Epoch:    310 loss= 4.578483 acc= 0.373767 cost_time= 0.303041s \n",
      "Epoch:    320 loss= 4.709248 acc= 0.388446 cost_time= 0.304471s \n",
      "Epoch:    330 loss= 4.123394 acc= 0.337712 cost_time= 0.303979s \n",
      "Epoch:    340 loss= 4.389384 acc= 0.368367 cost_time= 0.305865s \n",
      "Epoch:    350 loss= 4.374565 acc= 0.333599 cost_time= 0.304722s \n",
      "Epoch:    360 loss= 3.349967 acc= 0.320866 cost_time= 0.319192s \n",
      "Epoch:    370 loss= 4.209681 acc= 0.360711 cost_time= 0.306783s \n",
      "Epoch:    380 loss= 3.632422 acc= 0.317143 cost_time= 0.306962s \n",
      "Epoch:    390 loss= 4.399338 acc= 0.340253 cost_time= 0.304797s \n",
      "Epoch:    400 loss= 4.296392 acc= 0.327239 cost_time= 0.302467s \n",
      "Epoch:    410 loss= 4.118844 acc= 0.277612 cost_time= 0.307291s \n",
      "Epoch:    420 loss= 4.477085 acc= 0.274406 cost_time= 0.307813s \n",
      "Epoch:    430 loss= 3.704868 acc= 0.373024 cost_time= 0.307765s \n",
      "Epoch:    440 loss= 4.145891 acc= 0.340028 cost_time= 0.306611s \n",
      "Epoch:    450 loss= 4.827523 acc= 0.396887 cost_time= 0.305889s \n",
      "Epoch:    460 loss= 3.817818 acc= 0.382691 cost_time= 0.305736s \n",
      "Epoch:    470 loss= 3.654720 acc= 0.337459 cost_time= 0.305887s \n",
      "Epoch:    480 loss= 3.586765 acc= 0.347575 cost_time= 0.305631s \n",
      "Epoch:    490 loss= 4.276012 acc= 0.269264 cost_time= 0.303884s \n",
      "Epoch:    500 loss= 3.898808 acc= 0.313629 cost_time= 0.307070s \n",
      "Epoch:    510 loss= 4.077877 acc= 0.348185 cost_time= 0.303773s \n",
      "Epoch:    520 loss= 3.876070 acc= 0.314825 cost_time= 0.305979s \n",
      "Epoch:    530 loss= 3.554458 acc= 0.302284 cost_time= 0.303291s \n",
      "Epoch:    540 loss= 3.392830 acc= 0.355001 cost_time= 0.305322s \n",
      "Epoch:    550 loss= 3.229944 acc= 0.348394 cost_time= 0.302563s \n",
      "Epoch:    560 loss= 4.132102 acc= 0.271197 cost_time= 0.302182s \n",
      "Epoch:    570 loss= 3.749691 acc= 0.324176 cost_time= 0.303852s \n",
      "Epoch:    580 loss= 2.776037 acc= 0.384143 cost_time= 0.306883s \n",
      "Epoch:    590 loss= 3.947246 acc= 0.243729 cost_time= 0.302628s \n",
      "Epoch:    600 loss= 3.475600 acc= 0.322372 cost_time= 0.303836s \n",
      "Epoch:    610 loss= 4.309446 acc= 0.305328 cost_time= 0.305806s \n",
      "Epoch:    620 loss= 4.593970 acc= 0.306037 cost_time= 0.305900s \n",
      "Epoch:    630 loss= 3.770610 acc= 0.364714 cost_time= 0.305857s \n",
      "Epoch:    640 loss= 2.971169 acc= 0.408642 cost_time= 0.305160s \n",
      "Epoch:    650 loss= 4.093800 acc= 0.324708 cost_time= 0.304923s \n",
      "Epoch:    660 loss= 3.307563 acc= 0.405256 cost_time= 0.300719s \n",
      "Epoch:    670 loss= 3.939827 acc= 0.328953 cost_time= 0.303927s \n",
      "Epoch:    680 loss= 3.572902 acc= 0.373188 cost_time= 0.305482s \n",
      "Epoch:    690 loss= 3.348461 acc= 0.483990 cost_time= 0.305052s \n",
      "Epoch:    700 loss= 3.455065 acc= 0.357324 cost_time= 0.302594s \n",
      "Epoch:    710 loss= 4.334256 acc= 0.343319 cost_time= 0.305536s \n",
      "Epoch:    720 loss= 3.326294 acc= 0.397052 cost_time= 0.304573s \n",
      "Epoch:    730 loss= 3.995162 acc= 0.241726 cost_time= 0.301513s \n",
      "Epoch:    740 loss= 4.152779 acc= 0.292317 cost_time= 0.304782s \n",
      "Epoch:    750 loss= 4.499788 acc= 0.323425 cost_time= 0.305374s \n",
      "Epoch:    760 loss= 3.467675 acc= 0.299530 cost_time= 0.304137s \n",
      "Epoch:    770 loss= 4.190923 acc= 0.308461 cost_time= 0.305788s \n",
      "Epoch:    780 loss= 3.021637 acc= 0.377311 cost_time= 0.306971s \n",
      "Epoch:    790 loss= 3.845109 acc= 0.357393 cost_time= 0.302688s \n",
      "Epoch:    800 loss= 4.133248 acc= 0.341627 cost_time= 0.304361s \n",
      "Epoch:    810 loss= 4.516301 acc= 0.364631 cost_time= 0.304129s \n",
      "Epoch:    820 loss= 3.428050 acc= 0.406980 cost_time= 0.303767s \n",
      "Epoch:    830 loss= 3.752741 acc= 0.378637 cost_time= 0.304644s \n",
      "Epoch:    840 loss= 3.428291 acc= 0.425202 cost_time= 0.307515s \n",
      "Epoch:    850 loss= 3.935107 acc= 0.237034 cost_time= 0.305135s \n",
      "Epoch:    860 loss= 4.412593 acc= 0.302795 cost_time= 0.307209s \n",
      "Epoch:    870 loss= 3.444101 acc= 0.445217 cost_time= 0.303794s \n",
      "Epoch:    880 loss= 3.871769 acc= 0.348161 cost_time= 0.306460s \n",
      "Epoch:    890 loss= 3.666710 acc= 0.425831 cost_time= 0.302803s \n",
      "Epoch:    900 loss= 3.405142 acc= 0.288342 cost_time= 0.306580s \n",
      "Epoch:    910 loss= 3.493486 acc= 0.348649 cost_time= 0.305351s \n",
      "Epoch:    920 loss= 3.660440 acc= 0.335195 cost_time= 0.303342s \n",
      "Epoch:    930 loss= 3.972064 acc= 0.355851 cost_time= 0.305443s \n",
      "Epoch:    940 loss= 2.840298 acc= 0.292729 cost_time= 0.306437s \n",
      "Epoch:    950 loss= 3.152760 acc= 0.443900 cost_time= 0.303525s \n",
      "Epoch:    960 loss= 3.973810 acc= 0.308154 cost_time= 0.303714s \n",
      "Epoch:    970 loss= 4.312270 acc= 0.269910 cost_time= 0.304389s \n",
      "Epoch:    980 loss= 3.557268 acc= 0.324381 cost_time= 0.304269s \n",
      "Epoch:    990 loss= 4.033553 acc= 0.333729 cost_time= 0.305461s \n",
      "Epoch:   1000 loss= 4.235360 acc= 0.344244 cost_time= 0.309465s \n",
      "Epoch:   1010 loss= 4.697801 acc= 0.262821 cost_time= 0.307130s \n",
      "Epoch:   1020 loss= 4.139518 acc= 0.378963 cost_time= 0.308246s \n",
      "Epoch:   1030 loss= 3.060117 acc= 0.446098 cost_time= 0.303665s \n",
      "Epoch:   1040 loss= 3.353930 acc= 0.364880 cost_time= 0.304730s \n",
      "Epoch:   1050 loss= 3.741658 acc= 0.403548 cost_time= 0.302786s \n",
      "Epoch:   1060 loss= 3.541153 acc= 0.371444 cost_time= 0.304043s \n",
      "Epoch:   1070 loss= 3.477873 acc= 0.357613 cost_time= 0.303406s \n",
      "Epoch:   1080 loss= 3.774091 acc= 0.323702 cost_time= 0.304413s \n",
      "Epoch:   1090 loss= 3.825033 acc= 0.349483 cost_time= 0.303138s \n",
      "Epoch:   1100 loss= 3.304914 acc= 0.334866 cost_time= 0.304841s \n",
      "Epoch:   1110 loss= 3.452244 acc= 0.271753 cost_time= 0.313406s \n",
      "Epoch:   1120 loss= 3.064070 acc= 0.404431 cost_time= 0.314619s \n",
      "Epoch:   1130 loss= 3.913023 acc= 0.354798 cost_time= 0.316395s \n",
      "Epoch:   1140 loss= 3.157867 acc= 0.409069 cost_time= 0.311847s \n",
      "Epoch:   1150 loss= 3.723225 acc= 0.355191 cost_time= 0.314582s \n",
      "Epoch:   1160 loss= 4.447398 acc= 0.284378 cost_time= 0.307799s \n",
      "Epoch:   1170 loss= 3.707998 acc= 0.416027 cost_time= 0.302756s \n",
      "Epoch:   1180 loss= 3.835579 acc= 0.342253 cost_time= 0.306124s \n",
      "Epoch:   1190 loss= 3.219275 acc= 0.413790 cost_time= 0.305917s \n",
      "Epoch:   1200 loss= 3.021150 acc= 0.429307 cost_time= 0.305615s \n",
      "Epoch:   1210 loss= 3.889272 acc= 0.290877 cost_time= 0.305315s \n",
      "Epoch:   1220 loss= 3.733695 acc= 0.342501 cost_time= 0.302510s \n",
      "Epoch:   1230 loss= 3.821426 acc= 0.337730 cost_time= 0.303470s \n",
      "Epoch:   1240 loss= 3.637187 acc= 0.399154 cost_time= 0.303488s \n",
      "Epoch:   1250 loss= 3.935636 acc= 0.401218 cost_time= 0.303874s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1260 loss= 3.649605 acc= 0.409875 cost_time= 0.306531s \n",
      "Epoch:   1270 loss= 3.166276 acc= 0.400841 cost_time= 0.304499s \n",
      "Epoch:   1280 loss= 3.148452 acc= 0.392201 cost_time= 0.303509s \n",
      "Epoch:   1290 loss= 3.207541 acc= 0.415716 cost_time= 0.303999s \n",
      "Epoch:   1300 loss= 3.269592 acc= 0.413913 cost_time= 0.305458s \n",
      "Epoch:   1310 loss= 3.485842 acc= 0.350604 cost_time= 0.304806s \n",
      "Epoch:   1320 loss= 3.303066 acc= 0.263764 cost_time= 0.303361s \n",
      "Epoch:   1330 loss= 3.344885 acc= 0.411114 cost_time= 0.307120s \n",
      "Epoch:   1340 loss= 3.267272 acc= 0.368791 cost_time= 0.305807s \n",
      "Epoch:   1350 loss= 4.177493 acc= 0.331906 cost_time= 0.306005s \n",
      "Epoch:   1360 loss= 4.359249 acc= 0.300525 cost_time= 0.308492s \n",
      "Epoch:   1370 loss= 2.990879 acc= 0.439731 cost_time= 0.306023s \n",
      "Epoch:   1380 loss= 3.617879 acc= 0.369497 cost_time= 0.304046s \n",
      "Epoch:   1390 loss= 4.423100 acc= 0.320226 cost_time= 0.306660s \n",
      "Epoch:   1400 loss= 3.629274 acc= 0.295110 cost_time= 0.302627s \n",
      "Epoch:   1410 loss= 3.622029 acc= 0.380273 cost_time= 0.304795s \n",
      "Epoch:   1420 loss= 3.187180 acc= 0.411894 cost_time= 0.305126s \n",
      "Epoch:   1430 loss= 3.784539 acc= 0.278079 cost_time= 0.302314s \n",
      "Epoch:   1440 loss= 3.355664 acc= 0.459843 cost_time= 0.301987s \n",
      "Epoch:   1450 loss= 3.432824 acc= 0.425221 cost_time= 0.304722s \n",
      "Epoch:   1460 loss= 3.723932 acc= 0.335386 cost_time= 0.305699s \n",
      "Epoch:   1470 loss= 3.245025 acc= 0.318499 cost_time= 0.306971s \n",
      "Epoch:   1480 loss= 4.357810 acc= 0.293642 cost_time= 0.303740s \n",
      "Epoch:   1490 loss= 3.374767 acc= 0.360625 cost_time= 0.301634s \n",
      "Epoch:   1500 loss= 2.987914 acc= 0.358827 cost_time= 0.306087s \n",
      "Epoch:   1510 loss= 4.390045 acc= 0.384276 cost_time= 0.304105s \n",
      "Epoch:   1520 loss= 3.421722 acc= 0.343720 cost_time= 0.304543s \n",
      "Epoch:   1530 loss= 3.243374 acc= 0.463791 cost_time= 0.304846s \n",
      "Epoch:   1540 loss= 3.541925 acc= 0.306848 cost_time= 0.306908s \n",
      "Epoch:   1550 loss= 4.323481 acc= 0.310426 cost_time= 0.306177s \n",
      "Epoch:   1560 loss= 3.614890 acc= 0.345152 cost_time= 0.303350s \n",
      "Epoch:   1570 loss= 3.360724 acc= 0.351303 cost_time= 0.304419s \n",
      "Epoch:   1580 loss= 3.375281 acc= 0.368883 cost_time= 0.304599s \n",
      "Epoch:   1590 loss= 3.138277 acc= 0.367313 cost_time= 0.303942s \n",
      "Epoch:   1600 loss= 3.374903 acc= 0.371990 cost_time= 0.302204s \n",
      "Epoch:   1610 loss= 3.782207 acc= 0.306898 cost_time= 0.303439s \n",
      "Epoch:   1620 loss= 3.887057 acc= 0.370614 cost_time= 0.304104s \n",
      "Epoch:   1630 loss= 3.511632 acc= 0.333344 cost_time= 0.302760s \n",
      "Epoch:   1640 loss= 2.874126 acc= 0.476328 cost_time= 0.305879s \n",
      "Epoch:   1650 loss= 3.682559 acc= 0.307363 cost_time= 0.304055s \n",
      "Epoch:   1660 loss= 3.318101 acc= 0.354189 cost_time= 0.307102s \n",
      "Epoch:   1670 loss= 3.039591 acc= 0.286709 cost_time= 0.307576s \n",
      "Epoch:   1680 loss= 3.019053 acc= 0.410875 cost_time= 0.303627s \n",
      "Epoch:   1690 loss= 3.689641 acc= 0.374432 cost_time= 0.306342s \n",
      "Epoch:   1700 loss= 3.683716 acc= 0.386640 cost_time= 0.308402s \n",
      "Epoch:   1710 loss= 2.896484 acc= 0.332140 cost_time= 0.305203s \n",
      "Epoch:   1720 loss= 4.154540 acc= 0.266068 cost_time= 0.304421s \n",
      "Epoch:   1730 loss= 3.579885 acc= 0.336493 cost_time= 0.306750s \n",
      "Epoch:   1740 loss= 3.518278 acc= 0.451468 cost_time= 0.308195s \n",
      "Epoch:   1750 loss= 3.076605 acc= 0.402528 cost_time= 0.306927s \n",
      "Epoch:   1760 loss= 3.309566 acc= 0.284488 cost_time= 0.304141s \n",
      "Epoch:   1770 loss= 3.187806 acc= 0.360843 cost_time= 0.305321s \n",
      "Epoch:   1780 loss= 3.994459 acc= 0.370641 cost_time= 0.305659s \n",
      "Epoch:   1790 loss= 3.396493 acc= 0.420545 cost_time= 0.306940s \n",
      "Epoch:   1800 loss= 3.596853 acc= 0.417330 cost_time= 0.304238s \n",
      "Epoch:   1810 loss= 2.998195 acc= 0.401295 cost_time= 0.305496s \n",
      "Epoch:   1820 loss= 3.040978 acc= 0.422459 cost_time= 0.318445s \n",
      "Epoch:   1830 loss= 4.049152 acc= 0.356105 cost_time= 0.307868s \n",
      "Epoch:   1840 loss= 4.005593 acc= 0.324511 cost_time= 0.306604s \n",
      "Epoch:   1850 loss= 3.552739 acc= 0.362757 cost_time= 0.304507s \n",
      "Epoch:   1860 loss= 2.479968 acc= 0.454360 cost_time= 0.307655s \n",
      "Epoch:   1870 loss= 2.795032 acc= 0.275068 cost_time= 0.307350s \n",
      "Epoch:   1880 loss= 3.693197 acc= 0.372102 cost_time= 0.305565s \n",
      "Epoch:   1890 loss= 3.242857 acc= 0.342989 cost_time= 0.307893s \n",
      "Epoch:   1900 loss= 3.179789 acc= 0.345300 cost_time= 0.306834s \n",
      "Epoch:   1910 loss= 3.442929 acc= 0.321922 cost_time= 0.306258s \n",
      "Epoch:   1920 loss= 3.900654 acc= 0.385631 cost_time= 0.309040s \n",
      "Epoch:   1930 loss= 3.581549 acc= 0.488425 cost_time= 0.307682s \n",
      "Epoch:   1940 loss= 3.345699 acc= 0.411234 cost_time= 0.315941s \n",
      "Epoch:   1950 loss= 2.878758 acc= 0.379645 cost_time= 0.327811s \n",
      "Epoch:   1960 loss= 3.473796 acc= 0.415829 cost_time= 0.320168s \n",
      "Epoch:   1970 loss= 3.530229 acc= 0.383928 cost_time= 0.323649s \n",
      "Epoch:   1980 loss= 3.311171 acc= 0.379911 cost_time= 0.320984s \n",
      "Epoch:   1990 loss= 2.877859 acc= 0.340956 cost_time= 0.320753s \n",
      "Epoch:   2000 loss= 3.378568 acc= 0.397271 cost_time= 0.322615s \n",
      "Epoch:   2010 loss= 3.369889 acc= 0.372254 cost_time= 0.336779s \n",
      "Epoch:   2020 loss= 3.578592 acc= 0.285189 cost_time= 0.382233s \n",
      "Epoch:   2030 loss= 2.926823 acc= 0.388830 cost_time= 0.326752s \n",
      "Epoch:   2040 loss= 3.644335 acc= 0.385687 cost_time= 0.325166s \n",
      "Epoch:   2050 loss= 3.508573 acc= 0.420017 cost_time= 0.324376s \n",
      "Epoch:   2060 loss= 3.016536 acc= 0.301756 cost_time= 0.328845s \n",
      "Epoch:   2070 loss= 2.913505 acc= 0.398826 cost_time= 0.331204s \n",
      "Epoch:   2080 loss= 3.225008 acc= 0.355976 cost_time= 0.325774s \n",
      "Epoch:   2090 loss= 3.230690 acc= 0.418786 cost_time= 0.321963s \n",
      "Epoch:   2100 loss= 3.055058 acc= 0.317413 cost_time= 0.319684s \n",
      "Epoch:   2110 loss= 3.242337 acc= 0.376618 cost_time= 0.335607s \n",
      "Epoch:   2120 loss= 3.585919 acc= 0.434934 cost_time= 0.343314s \n",
      "Epoch:   2130 loss= 3.789488 acc= 0.325555 cost_time= 0.329445s \n",
      "Epoch:   2140 loss= 3.059056 acc= 0.419237 cost_time= 0.324416s \n",
      "Epoch:   2150 loss= 3.543031 acc= 0.327449 cost_time= 0.330904s \n",
      "Epoch:   2160 loss= 3.004159 acc= 0.446787 cost_time= 0.321491s \n",
      "Epoch:   2170 loss= 3.354706 acc= 0.472477 cost_time= 0.326120s \n",
      "Epoch:   2180 loss= 3.399697 acc= 0.369764 cost_time= 0.324866s \n",
      "Epoch:   2190 loss= 3.815086 acc= 0.354389 cost_time= 0.326176s \n",
      "Epoch:   2200 loss= 3.178796 acc= 0.362242 cost_time= 0.318499s \n",
      "Epoch:   2210 loss= 3.352738 acc= 0.434507 cost_time= 0.332627s \n",
      "Epoch:   2220 loss= 2.729822 acc= 0.520078 cost_time= 0.334444s \n",
      "Epoch:   2230 loss= 3.404152 acc= 0.398832 cost_time= 0.325416s \n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    # sess init or restore\n",
    "    if weights_path is None:\n",
    "        log.info('Training from scratch')\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # read vgg model\n",
    "        pretrained_weights = np.load('./data/vgg16.npy', encoding='latin1').item()\n",
    "        for vv in tf.trainable_variables():\n",
    "            weights_key = vv.name.split('/')[-3]\n",
    "            try:\n",
    "                weights = pretrained_weights[weights_key][0]\n",
    "                _op = tf.assign(vv, weights)\n",
    "                sess.run(_op)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    else:\n",
    "        log.info('Restore model from last model checkpoint {:s}'.format(weights_path))\n",
    "        # restore weights\n",
    "        saver.restore(sess=sess, save_path=weights_path)\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(CFG.TRAIN.EPOCHS):\n",
    "\n",
    "        # Use training data for optimization\n",
    "        for _ in range(CFG.TRAIN.NOOFBATCHES):\n",
    "            with tf.device('/cpu:0'):\n",
    "                gt_imgs, binary_gt_labels = train_dataset.next_batch(CFG.TRAIN.BATCH_SIZE)\n",
    "                gt_imgs, binary_gt_labels = resize_training_data(gt_imgs, binary_gt_labels)\n",
    "\n",
    "            sess.run(optimizer, feed_dict={input_tensor:gt_imgs, \n",
    "                                           binary_label:binary_gt_labels, \n",
    "                                           phase:'train'})\n",
    "\n",
    "        # Validate after every epoch\n",
    "        t_start = time.time()\n",
    "        with tf.device('/cpu:0'):\n",
    "            gt_imgs, binary_gt_labels = train_dataset.next_batch(CFG.TRAIN.BATCH_SIZE)\n",
    "            gt_imgs, binary_gt_labels = resize_training_data(gt_imgs, binary_gt_labels)\n",
    "\n",
    "        train_loss, train_accuracy, train_img, train_summary = \\\n",
    "            sess.run([loss, accuracy, out_logits_out, merged_summary_op],\n",
    "                     feed_dict={input_tensor: gt_imgs,\n",
    "                                binary_label: binary_gt_labels,\n",
    "                                phase: 'train'})\n",
    "\n",
    "        # time\n",
    "        cost_time = time.time() - t_start\n",
    "        train_cost_time_mean.append(cost_time)\n",
    "        \n",
    "        # summary\n",
    "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
    "\n",
    "        # history\n",
    "        if epoch*CFG.TRAIN.NOOFBATCHES > 10:\n",
    "            history.append([train_loss, train_accuracy])\n",
    "\n",
    "        # progress\n",
    "        if epoch*CFG.TRAIN.NOOFBATCHES % 100 == 0:\n",
    "            print('Epoch: {:6d} loss= {:6f} acc= {:6f} cost_time= {:5f}s '.\n",
    "                  format(epoch, train_loss, train_accuracy, np.mean(train_cost_time_mean)))\n",
    "            train_cost_time_mean.clear()\n",
    "\n",
    "        # output image\n",
    "        if epoch*CFG.TRAIN.NOOFBATCHES % 100 == 0:\n",
    "            binary_seg_image_3ch = np.array([[[0]*3]*__C.TRAIN.IMG_WIDTH]*__C.TRAIN.IMG_HEIGHT, np.float64)\n",
    "            binary_seg_image_3ch[:, :, 0] = 0\n",
    "            binary_seg_image_3ch[:, :, 1] = 0\n",
    "            binary_seg_image_3ch[:, :, 2] = train_img[0][:, :]*255\n",
    "            image = gt_imgs[0] + VGG_MEAN\n",
    "            image_field2 = cv2.addWeighted(image, 1.0, binary_seg_image_3ch, 1.0, 0.0)\n",
    "            path = image_save_path + '/image_{:d}.png'.format(epoch)\n",
    "            cv2.imwrite(path, image_field2)\n",
    "\n",
    "        # store model\n",
    "        if epoch*CFG.TRAIN.NOOFBATCHES % 2000 == 0:\n",
    "            saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n",
    "\n",
    "# store model\n",
    "saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "fig = plt.figure(figsize=(15,15),dpi=100)\n",
    "ax1 = fig.add_subplot(2,1,1, facecolor='w')\n",
    "ax1.plot(np.arange(1, history.shape[0] + 1), history[:, 0], label='loss')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.arange(1, history.shape[0] + 1), history[:, 1], label='accuracy', color=cmap(1))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
